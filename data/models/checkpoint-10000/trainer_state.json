{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.734288081429755,
  "eval_steps": 500,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04734288081429755,
      "grad_norm": 2.5649120807647705,
      "learning_rate": 9.90530303030303e-05,
      "loss": 0.6546,
      "step": 100
    },
    {
      "epoch": 0.0946857616285951,
      "grad_norm": 2.0964274406433105,
      "learning_rate": 9.810606060606061e-05,
      "loss": 0.4743,
      "step": 200
    },
    {
      "epoch": 0.14202864244289265,
      "grad_norm": 3.312141180038452,
      "learning_rate": 9.71590909090909e-05,
      "loss": 0.4504,
      "step": 300
    },
    {
      "epoch": 0.1893715232571902,
      "grad_norm": 2.358806848526001,
      "learning_rate": 9.621212121212123e-05,
      "loss": 0.435,
      "step": 400
    },
    {
      "epoch": 0.23671440407148775,
      "grad_norm": 2.271996259689331,
      "learning_rate": 9.526515151515152e-05,
      "loss": 0.4414,
      "step": 500
    },
    {
      "epoch": 0.2840572848857853,
      "grad_norm": 2.631307363510132,
      "learning_rate": 9.431818181818182e-05,
      "loss": 0.4127,
      "step": 600
    },
    {
      "epoch": 0.33140016570008285,
      "grad_norm": 2.5761239528656006,
      "learning_rate": 9.337121212121213e-05,
      "loss": 0.3978,
      "step": 700
    },
    {
      "epoch": 0.3787430465143804,
      "grad_norm": 3.101418972015381,
      "learning_rate": 9.242424242424242e-05,
      "loss": 0.4052,
      "step": 800
    },
    {
      "epoch": 0.42608592732867795,
      "grad_norm": 2.2550816535949707,
      "learning_rate": 9.147727272727274e-05,
      "loss": 0.3883,
      "step": 900
    },
    {
      "epoch": 0.4734288081429755,
      "grad_norm": 3.1455037593841553,
      "learning_rate": 9.053030303030303e-05,
      "loss": 0.4008,
      "step": 1000
    },
    {
      "epoch": 0.5207716889572731,
      "grad_norm": 1.9707932472229004,
      "learning_rate": 8.958333333333335e-05,
      "loss": 0.3887,
      "step": 1100
    },
    {
      "epoch": 0.5681145697715706,
      "grad_norm": 1.8268717527389526,
      "learning_rate": 8.863636363636364e-05,
      "loss": 0.3816,
      "step": 1200
    },
    {
      "epoch": 0.6154574505858682,
      "grad_norm": 2.5707509517669678,
      "learning_rate": 8.768939393939394e-05,
      "loss": 0.3725,
      "step": 1300
    },
    {
      "epoch": 0.6628003314001657,
      "grad_norm": 2.329531669616699,
      "learning_rate": 8.674242424242425e-05,
      "loss": 0.3833,
      "step": 1400
    },
    {
      "epoch": 0.7101432122144633,
      "grad_norm": 3.337599277496338,
      "learning_rate": 8.579545454545454e-05,
      "loss": 0.3686,
      "step": 1500
    },
    {
      "epoch": 0.7574860930287608,
      "grad_norm": 2.204998016357422,
      "learning_rate": 8.484848484848486e-05,
      "loss": 0.3696,
      "step": 1600
    },
    {
      "epoch": 0.8048289738430584,
      "grad_norm": 2.026099443435669,
      "learning_rate": 8.390151515151515e-05,
      "loss": 0.3659,
      "step": 1700
    },
    {
      "epoch": 0.8521718546573559,
      "grad_norm": 2.3356406688690186,
      "learning_rate": 8.295454545454547e-05,
      "loss": 0.3597,
      "step": 1800
    },
    {
      "epoch": 0.8995147354716535,
      "grad_norm": 1.8152722120285034,
      "learning_rate": 8.200757575757576e-05,
      "loss": 0.3411,
      "step": 1900
    },
    {
      "epoch": 0.946857616285951,
      "grad_norm": 2.7642269134521484,
      "learning_rate": 8.106060606060607e-05,
      "loss": 0.3577,
      "step": 2000
    },
    {
      "epoch": 0.9942004971002486,
      "grad_norm": 2.722923755645752,
      "learning_rate": 8.011363636363637e-05,
      "loss": 0.3698,
      "step": 2100
    },
    {
      "epoch": 1.0415433779145462,
      "grad_norm": 3.2262890338897705,
      "learning_rate": 7.916666666666666e-05,
      "loss": 0.3048,
      "step": 2200
    },
    {
      "epoch": 1.0888862587288437,
      "grad_norm": 2.5072901248931885,
      "learning_rate": 7.821969696969698e-05,
      "loss": 0.2978,
      "step": 2300
    },
    {
      "epoch": 1.1362291395431412,
      "grad_norm": 1.9063173532485962,
      "learning_rate": 7.727272727272727e-05,
      "loss": 0.2874,
      "step": 2400
    },
    {
      "epoch": 1.1835720203574387,
      "grad_norm": 3.053363084793091,
      "learning_rate": 7.632575757575758e-05,
      "loss": 0.2893,
      "step": 2500
    },
    {
      "epoch": 1.2309149011717362,
      "grad_norm": 2.8325822353363037,
      "learning_rate": 7.537878787878788e-05,
      "loss": 0.2908,
      "step": 2600
    },
    {
      "epoch": 1.278257781986034,
      "grad_norm": 2.2254042625427246,
      "learning_rate": 7.443181818181817e-05,
      "loss": 0.2777,
      "step": 2700
    },
    {
      "epoch": 1.3256006628003314,
      "grad_norm": 2.8054473400115967,
      "learning_rate": 7.348484848484849e-05,
      "loss": 0.2981,
      "step": 2800
    },
    {
      "epoch": 1.372943543614629,
      "grad_norm": 2.319160223007202,
      "learning_rate": 7.253787878787878e-05,
      "loss": 0.2897,
      "step": 2900
    },
    {
      "epoch": 1.4202864244289266,
      "grad_norm": 3.218339204788208,
      "learning_rate": 7.15909090909091e-05,
      "loss": 0.3002,
      "step": 3000
    },
    {
      "epoch": 1.467629305243224,
      "grad_norm": 3.247087001800537,
      "learning_rate": 7.06439393939394e-05,
      "loss": 0.2928,
      "step": 3100
    },
    {
      "epoch": 1.5149721860575216,
      "grad_norm": 2.3666346073150635,
      "learning_rate": 6.96969696969697e-05,
      "loss": 0.2914,
      "step": 3200
    },
    {
      "epoch": 1.562315066871819,
      "grad_norm": 2.1762452125549316,
      "learning_rate": 6.875e-05,
      "loss": 0.3003,
      "step": 3300
    },
    {
      "epoch": 1.6096579476861166,
      "grad_norm": 1.8986003398895264,
      "learning_rate": 6.78030303030303e-05,
      "loss": 0.2842,
      "step": 3400
    },
    {
      "epoch": 1.6570008285004143,
      "grad_norm": 2.953423261642456,
      "learning_rate": 6.685606060606061e-05,
      "loss": 0.2971,
      "step": 3500
    },
    {
      "epoch": 1.7043437093147118,
      "grad_norm": 2.846033811569214,
      "learning_rate": 6.59090909090909e-05,
      "loss": 0.2882,
      "step": 3600
    },
    {
      "epoch": 1.7516865901290095,
      "grad_norm": 2.5489869117736816,
      "learning_rate": 6.496212121212122e-05,
      "loss": 0.2829,
      "step": 3700
    },
    {
      "epoch": 1.799029470943307,
      "grad_norm": 3.2076127529144287,
      "learning_rate": 6.401515151515152e-05,
      "loss": 0.2733,
      "step": 3800
    },
    {
      "epoch": 1.8463723517576045,
      "grad_norm": 2.699892520904541,
      "learning_rate": 6.306818181818182e-05,
      "loss": 0.2763,
      "step": 3900
    },
    {
      "epoch": 1.893715232571902,
      "grad_norm": 2.722399950027466,
      "learning_rate": 6.212121212121213e-05,
      "loss": 0.287,
      "step": 4000
    },
    {
      "epoch": 1.9410581133861995,
      "grad_norm": 3.656860113143921,
      "learning_rate": 6.117424242424242e-05,
      "loss": 0.2784,
      "step": 4100
    },
    {
      "epoch": 1.988400994200497,
      "grad_norm": 2.332082509994507,
      "learning_rate": 6.022727272727273e-05,
      "loss": 0.2886,
      "step": 4200
    },
    {
      "epoch": 2.0357438750147945,
      "grad_norm": 2.43109130859375,
      "learning_rate": 5.928030303030303e-05,
      "loss": 0.2276,
      "step": 4300
    },
    {
      "epoch": 2.0830867558290924,
      "grad_norm": 2.828019618988037,
      "learning_rate": 5.833333333333334e-05,
      "loss": 0.2154,
      "step": 4400
    },
    {
      "epoch": 2.13042963664339,
      "grad_norm": 2.658968687057495,
      "learning_rate": 5.738636363636364e-05,
      "loss": 0.2231,
      "step": 4500
    },
    {
      "epoch": 2.1777725174576874,
      "grad_norm": 2.4366517066955566,
      "learning_rate": 5.643939393939395e-05,
      "loss": 0.224,
      "step": 4600
    },
    {
      "epoch": 2.225115398271985,
      "grad_norm": 2.84991455078125,
      "learning_rate": 5.549242424242425e-05,
      "loss": 0.2195,
      "step": 4700
    },
    {
      "epoch": 2.2724582790862824,
      "grad_norm": 3.1213860511779785,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 0.2215,
      "step": 4800
    },
    {
      "epoch": 2.31980115990058,
      "grad_norm": 2.803926706314087,
      "learning_rate": 5.359848484848485e-05,
      "loss": 0.2153,
      "step": 4900
    },
    {
      "epoch": 2.3671440407148774,
      "grad_norm": 4.449307918548584,
      "learning_rate": 5.265151515151515e-05,
      "loss": 0.22,
      "step": 5000
    },
    {
      "epoch": 2.414486921529175,
      "grad_norm": 4.047604560852051,
      "learning_rate": 5.170454545454546e-05,
      "loss": 0.2258,
      "step": 5100
    },
    {
      "epoch": 2.4618298023434724,
      "grad_norm": 2.683755397796631,
      "learning_rate": 5.075757575757576e-05,
      "loss": 0.2217,
      "step": 5200
    },
    {
      "epoch": 2.5091726831577703,
      "grad_norm": 3.9937992095947266,
      "learning_rate": 4.9810606060606065e-05,
      "loss": 0.2173,
      "step": 5300
    },
    {
      "epoch": 2.556515563972068,
      "grad_norm": 2.4835846424102783,
      "learning_rate": 4.886363636363637e-05,
      "loss": 0.2184,
      "step": 5400
    },
    {
      "epoch": 2.6038584447863653,
      "grad_norm": 3.4272618293762207,
      "learning_rate": 4.791666666666667e-05,
      "loss": 0.2178,
      "step": 5500
    },
    {
      "epoch": 2.651201325600663,
      "grad_norm": 4.00173807144165,
      "learning_rate": 4.696969696969697e-05,
      "loss": 0.2217,
      "step": 5600
    },
    {
      "epoch": 2.6985442064149603,
      "grad_norm": 5.1891560554504395,
      "learning_rate": 4.602272727272727e-05,
      "loss": 0.2196,
      "step": 5700
    },
    {
      "epoch": 2.745887087229258,
      "grad_norm": 2.893018960952759,
      "learning_rate": 4.5075757575757577e-05,
      "loss": 0.22,
      "step": 5800
    },
    {
      "epoch": 2.7932299680435557,
      "grad_norm": 2.7105140686035156,
      "learning_rate": 4.412878787878788e-05,
      "loss": 0.221,
      "step": 5900
    },
    {
      "epoch": 2.840572848857853,
      "grad_norm": 3.356877088546753,
      "learning_rate": 4.318181818181819e-05,
      "loss": 0.2329,
      "step": 6000
    },
    {
      "epoch": 2.8879157296721507,
      "grad_norm": 2.0178909301757812,
      "learning_rate": 4.2234848484848485e-05,
      "loss": 0.2155,
      "step": 6100
    },
    {
      "epoch": 2.935258610486448,
      "grad_norm": 3.2145421504974365,
      "learning_rate": 4.128787878787879e-05,
      "loss": 0.2254,
      "step": 6200
    },
    {
      "epoch": 2.9826014913007457,
      "grad_norm": 4.648599624633789,
      "learning_rate": 4.034090909090909e-05,
      "loss": 0.2259,
      "step": 6300
    },
    {
      "epoch": 3.029944372115043,
      "grad_norm": 2.831587791442871,
      "learning_rate": 3.939393939393939e-05,
      "loss": 0.185,
      "step": 6400
    },
    {
      "epoch": 3.0772872529293407,
      "grad_norm": 3.1362695693969727,
      "learning_rate": 3.84469696969697e-05,
      "loss": 0.1684,
      "step": 6500
    },
    {
      "epoch": 3.124630133743638,
      "grad_norm": 3.491438150405884,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.1724,
      "step": 6600
    },
    {
      "epoch": 3.1719730145579357,
      "grad_norm": 2.751671314239502,
      "learning_rate": 3.655303030303031e-05,
      "loss": 0.1687,
      "step": 6700
    },
    {
      "epoch": 3.219315895372233,
      "grad_norm": 3.2956864833831787,
      "learning_rate": 3.560606060606061e-05,
      "loss": 0.1681,
      "step": 6800
    },
    {
      "epoch": 3.266658776186531,
      "grad_norm": 6.593284606933594,
      "learning_rate": 3.465909090909091e-05,
      "loss": 0.1649,
      "step": 6900
    },
    {
      "epoch": 3.3140016570008286,
      "grad_norm": 3.2222399711608887,
      "learning_rate": 3.371212121212121e-05,
      "loss": 0.1705,
      "step": 7000
    },
    {
      "epoch": 3.361344537815126,
      "grad_norm": 3.377819061279297,
      "learning_rate": 3.2765151515151515e-05,
      "loss": 0.1643,
      "step": 7100
    },
    {
      "epoch": 3.4086874186294236,
      "grad_norm": 4.256266117095947,
      "learning_rate": 3.181818181818182e-05,
      "loss": 0.1623,
      "step": 7200
    },
    {
      "epoch": 3.456030299443721,
      "grad_norm": 3.100708246231079,
      "learning_rate": 3.0871212121212125e-05,
      "loss": 0.1689,
      "step": 7300
    },
    {
      "epoch": 3.5033731802580186,
      "grad_norm": 2.291961669921875,
      "learning_rate": 2.9924242424242427e-05,
      "loss": 0.1717,
      "step": 7400
    },
    {
      "epoch": 3.5507160610723165,
      "grad_norm": 3.1081125736236572,
      "learning_rate": 2.8977272727272732e-05,
      "loss": 0.1722,
      "step": 7500
    },
    {
      "epoch": 3.598058941886614,
      "grad_norm": 3.8924381732940674,
      "learning_rate": 2.803030303030303e-05,
      "loss": 0.1699,
      "step": 7600
    },
    {
      "epoch": 3.6454018227009115,
      "grad_norm": 3.2462944984436035,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.1635,
      "step": 7700
    },
    {
      "epoch": 3.692744703515209,
      "grad_norm": 3.968723773956299,
      "learning_rate": 2.6136363636363637e-05,
      "loss": 0.1638,
      "step": 7800
    },
    {
      "epoch": 3.7400875843295065,
      "grad_norm": 2.9584672451019287,
      "learning_rate": 2.518939393939394e-05,
      "loss": 0.1686,
      "step": 7900
    },
    {
      "epoch": 3.787430465143804,
      "grad_norm": 2.8244001865386963,
      "learning_rate": 2.4242424242424244e-05,
      "loss": 0.1618,
      "step": 8000
    },
    {
      "epoch": 3.8347733459581015,
      "grad_norm": 3.4814798831939697,
      "learning_rate": 2.3295454545454546e-05,
      "loss": 0.1736,
      "step": 8100
    },
    {
      "epoch": 3.882116226772399,
      "grad_norm": 2.4679625034332275,
      "learning_rate": 2.234848484848485e-05,
      "loss": 0.1698,
      "step": 8200
    },
    {
      "epoch": 3.9294591075866965,
      "grad_norm": 2.428805112838745,
      "learning_rate": 2.1401515151515152e-05,
      "loss": 0.1607,
      "step": 8300
    },
    {
      "epoch": 3.976801988400994,
      "grad_norm": 3.068310499191284,
      "learning_rate": 2.0454545454545457e-05,
      "loss": 0.1652,
      "step": 8400
    },
    {
      "epoch": 4.0241448692152915,
      "grad_norm": 3.9018380641937256,
      "learning_rate": 1.950757575757576e-05,
      "loss": 0.1453,
      "step": 8500
    },
    {
      "epoch": 4.071487750029589,
      "grad_norm": 2.387319326400757,
      "learning_rate": 1.856060606060606e-05,
      "loss": 0.1265,
      "step": 8600
    },
    {
      "epoch": 4.1188306308438865,
      "grad_norm": 2.997422933578491,
      "learning_rate": 1.7613636363636366e-05,
      "loss": 0.1247,
      "step": 8700
    },
    {
      "epoch": 4.166173511658185,
      "grad_norm": 4.4716081619262695,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.1275,
      "step": 8800
    },
    {
      "epoch": 4.213516392472482,
      "grad_norm": 2.1592202186584473,
      "learning_rate": 1.571969696969697e-05,
      "loss": 0.1259,
      "step": 8900
    },
    {
      "epoch": 4.26085927328678,
      "grad_norm": 2.4810259342193604,
      "learning_rate": 1.4772727272727274e-05,
      "loss": 0.1273,
      "step": 9000
    },
    {
      "epoch": 4.308202154101077,
      "grad_norm": 3.680180788040161,
      "learning_rate": 1.3825757575757576e-05,
      "loss": 0.128,
      "step": 9100
    },
    {
      "epoch": 4.355545034915375,
      "grad_norm": 6.020646095275879,
      "learning_rate": 1.287878787878788e-05,
      "loss": 0.1268,
      "step": 9200
    },
    {
      "epoch": 4.402887915729672,
      "grad_norm": 3.3376410007476807,
      "learning_rate": 1.1931818181818183e-05,
      "loss": 0.1203,
      "step": 9300
    },
    {
      "epoch": 4.45023079654397,
      "grad_norm": 4.4943695068359375,
      "learning_rate": 1.0984848484848486e-05,
      "loss": 0.1247,
      "step": 9400
    },
    {
      "epoch": 4.497573677358267,
      "grad_norm": 4.456182479858398,
      "learning_rate": 1.0037878787878788e-05,
      "loss": 0.1241,
      "step": 9500
    },
    {
      "epoch": 4.544916558172565,
      "grad_norm": 3.197742462158203,
      "learning_rate": 9.090909090909091e-06,
      "loss": 0.1251,
      "step": 9600
    },
    {
      "epoch": 4.592259438986862,
      "grad_norm": 4.551591873168945,
      "learning_rate": 8.143939393939394e-06,
      "loss": 0.1226,
      "step": 9700
    },
    {
      "epoch": 4.63960231980116,
      "grad_norm": 2.6137866973876953,
      "learning_rate": 7.196969696969698e-06,
      "loss": 0.1156,
      "step": 9800
    },
    {
      "epoch": 4.686945200615457,
      "grad_norm": 2.905336618423462,
      "learning_rate": 6.25e-06,
      "loss": 0.1236,
      "step": 9900
    },
    {
      "epoch": 4.734288081429755,
      "grad_norm": 3.06284761428833,
      "learning_rate": 5.303030303030304e-06,
      "loss": 0.1232,
      "step": 10000
    }
  ],
  "logging_steps": 100,
  "max_steps": 10560,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.60662733847849e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
